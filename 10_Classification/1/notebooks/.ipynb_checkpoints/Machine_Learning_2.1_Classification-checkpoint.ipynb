{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import rcParams\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Books:**\n",
    "\n",
    "Introduction to Statistical Learning:\n",
    "http://faculty.marshall.usc.edu/gareth-james/ISL/ISLR%20Seventh%20Printing.pdf\n",
    "\n",
    "Hands on Machine Learning with scikit-learn and tensorflow:\n",
    "https://www.lpsm.paris/pageperso/has/source/Hand-on-ML.pdf\n",
    "\n",
    "\n",
    "**Scikit-learn doc:**\n",
    "\n",
    "https://scikit-learn.org/stable/tutorial/basic/tutorial.html\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/classification/plot_classifier_comparison.html\n",
    "\n",
    "\n",
    "**Other resources:**\n",
    "\n",
    "https://www.youtube.com/watch?v=UqYde-LULfs\n",
    "\n",
    "https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc\n",
    "\n",
    "https://medium.com/30-days-of-machine-learning/day-3-k-nearest-neighbors-and-bias-variance-tradeoff-75f84d515bdb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classification with scikit-learn : predicting heart diseases with machine learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Heart disease dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For discovering classification, we're gonna use a rather classic dataset: the heart disease dataset. This dataset contains 13 symptoms and other attributes of patients that have been checked for a heart disease, such as their age, their cholesterol blood level, the type of pain they report, .... observed on 303 patients. These 13 attributes are called the *features*. And for each of these patients, we know if they have a heart disease or not: the field called *target* in the dataset. This field is generally called the *classes* of the problem, in this case we have two classes : 0 for healthy patients, and 1 for patients with a heart disease, hence it is called a *binary classification problem*.\n",
    "\n",
    "As with linear regression, we generally denote the feature matrix by $X$, and the classes with $y$, where, in this dataset:\n",
    "\n",
    "<div style=\"font-size: 150%;\" align= \"center\"> \n",
    "$X \\in \\mathbb{R}^{303 \\times 13}$, $y \\in  \\{0,1\\}^{303}.$\n",
    "</div>\n",
    "\n",
    "The goal of classification is to learn a function, or *classifier*, $f$ that approximates the true classes $y$ from the features $X$:\n",
    "\n",
    "<div style=\"font-size: 150%;\" align= \"center\"> \n",
    "$y \\approx f(X)$\n",
    "</div>\n",
    "\n",
    "However as $y$ is categorical in classification problems since it represents different classes, as opposed to a continuous value as in linear regression, we cannot use linear regression for solving these problems, and we need different models. \n",
    "\n",
    "Also note that classification is not always binary, and can classify features between more than 2 classes (will not be covered today, but the models that we will use here can be directly used with any number of classes).\n",
    "\n",
    "To get a better idea of these concepts, read chapter 4.1 and 4.2 from the *Introduction to Statistical Learning* book."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the dataset into a pandas DataFrame:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = pd.read_csv('../data/heart.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's have a look at the column and content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are the 13 features, and the classes assignation column called here *target*. We can see that there is no missing values in the data, so far so good. Their names correspond to the following clinical observations:\n",
    "\n",
    "|Name |Type |\tDescription |\n",
    "|-----|-----|---------------|\n",
    "|age      |integer | age of patient |\n",
    "|sex      |integer | 1=male; 0=female |\n",
    "|cp       |integer | chest pain type: 0=typical angina; 1=atypical angine; 2=non-anginal pain; 3=asymptomatic |\n",
    "|trestbps |integer | resting blood pressure (mm Hg) |\n",
    "|chol     |integer | serum cholestrol (mg/dl) |\n",
    "|fbs      |integer | fasting blood sugar: 1 if > 120 mg/dl; 0 otherwise |\n",
    "|restecg  |integer | resting electrocardiographic results: 0=normal; 1=having ST-T wave abnormality; 2=showing probable or definite left ventricular hypertrophy |\n",
    "|thalach  |integer | maximum heart rate achieved |\n",
    "|exang    |integer | exercise induced angina: 1=yes; 0=no |\n",
    "|oldpeak  |float   | ST depression induced by exercise relative to rest |\n",
    "|slope    |integer | the slope of the peak exercise ST segment: 0=upsloping; 1=flat; 2=downsloping |\n",
    "|ca       |integer | number of major vessels (0-4) colored by flourosopy |\n",
    "|thal     |integer | 1=normal; 2=fixed defect; 3=reversable defect |\n",
    "|target      |integer | predicted attribute; 0=NO HEART DISEASE; 1=HEART DISEASE |\n",
    "\n",
    "Some of these features have continuous values such as *age* or *chol*, while others are categorical such as *cp* or *slope*. We will see that categorical variables need to be handled differently when preprocessing the data.\n",
    "\n",
    "This dataset is derived from : https://archive.ics.uci.edu/ml/datasets/Heart+Disease"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting into the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now have a look of the distribution of the values of each columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "dataset.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can see that the distribution of the features are different: the age vary from 29 to 77, whereas cholesterol rates from 126 to 564. To avoid some features to bias the classifier because they have larger values than others, we will need to center their means to 0 and scale their variance to 1 when preprocessing them.\n",
    "Let's now plot the histogram of each feature :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 16,12\n",
    "plots = dataset.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It is quite clear now that some of the features are categorical with peaks only at some value, whereas others are continuous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Another important thing to check when performing classification is the balance between the target classes: is there as many samples (=patients) that are sick as there are healthy patient:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 8,6\n",
    "plt.bar(dataset['target'].unique(), dataset['target'].value_counts(), color = ['red', 'green'])\n",
    "plt.xticks([0, 1])\n",
    "plt.xlabel('Target Classes')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Count of each Target Class')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A too much imbalanced dataset can bias the classification model towards the class with more samples. Here we see that the two classes, healthy and having a heart diseased, are quite balanced in this dataset. (Some classification models allow for correcting this if needed, you can later have a look at : https://scikit-learn.org/stable/auto_examples/svm/plot_separating_hyperplane_unbalanced.html , but we won't need it today)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also plot the correlation matrix beween all the features and the target classes to get an idea of which features will probably be good predictors for our problem :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Some correlate with target, others not\n",
    "rcParams['figure.figsize'] = 20, 14\n",
    "plt.matshow(dataset.corr())\n",
    "plt.yticks(np.arange(dataset.shape[1]), dataset.columns)\n",
    "plt.xticks(np.arange(dataset.shape[1]), dataset.columns)\n",
    "plt.colorbar()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we can see that no single feature has a very high or very low correlation with the target value, meaning that the diagnostic is complex and will require a combination of all of these."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have seen that we have categorical variables, for example the *cp* variable that describes the chest pain type reported with number from 0 to 3:\n",
    "0=typical angina; 1=atypical angina; 2=non-anginal pain; 3=asymptomatic.\n",
    "\n",
    "However these are *qualitative* categories, and there is no notion of distance between these numbers: an atypical angina is not 2 times a typical angina! \n",
    "\n",
    "To handle that we are going to create what we call *dummy variables*, that is, for each value 0, 1, 2, and 3, we are going to create a new feature which is a 0-1 feature only, and has a 1 value only for its own value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_features = ['sex', 'cp', 'fbs', 'restecg', 'exang', 'slope', 'ca', 'thal']\n",
    "continuous_features = ['age', 'trestbps', 'chol', 'thalach', 'oldpeak']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create dummy variables, have a look at the get_dummies function from pandas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOFILL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's now plot our new features to see the difference:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rcParams['figure.figsize'] = 16,12\n",
    "plots = dataset.hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We've also said that we should center and scale our continuous variables to avoid biasing the classification model, use the *StandardScaler* class from scikit-learn to do so:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "#Scaling continuous variables\n",
    "\n",
    "#TOFILL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally our data is ready, let's separate the features from the classes in an $X$ and $y$ variables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TO FILL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the memo.txt file to answer the following question:\n",
    "\n",
    "1) What are the necessary preprocessing steps regarding:\n",
    "\n",
    "a) classes:\n",
    "\n",
    "b) categorical features:\n",
    "\n",
    "c) continuous features:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Classification with k-nearest neighbors, and classification metrics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "One easy introduction to classification is to start with the k-neareast neighbors method (KNN). At training, it simply memorizes all the training samples features $X$ and classes $y$. At test time given the features of one sample $x'$, it identifies the $k$ training samples $x_i, i \\in 1,\\dots,k$ that are the closest to $x'$ (in euclidian distance), and assign the class $y'$ that is the most frequent among the k-neareast neighbor classes $y_i, i \\in 1,\\dots,k$.\n",
    "\n",
    "So each test sample is assigned a probablity, for example of having a heart disease:\n",
    "\n",
    "<div style=\"font-size: 150%;\" align= \"center\"> \n",
    "$P(y' = 1 ) = \\frac{1}{k} \\sum_{ i \\in 1,\\dots,k} \\mathbb{1}(y_i = 1) $\n",
    "</div>\n",
    "\n",
    "where the indicator function $\\mathbb{1}(y_i = 1) = 1$ if $y_i = 1$, else $\\mathbb{1}(y_i = 1) = 0$. So the probability of having a heart disease is the proportion of the k-nearest train samples that have a heart disease.\n",
    "\n",
    "For an intuitive explanation of KNNs, watch the first 1:45min of this vid (the rest of it that is about Voronoi partitions is not necessary): https://www.youtube.com/watch?v=UqYde-LULfs\n",
    "\n",
    "You can also read pages 38-40 from the *Introduction to Statistical Learning*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You have already seen during the previous lessons that doing a single train/test split is not a very good estimation of the predictive power of the regression, and that cross-validation should always be preferred. It is the same with classification, so read about the imported classes below, and use them compute the predictions over the whole dataset by doing a 10-fold cross-validation on a k-nearest neighbors classifier, with k=15 :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://scikit-learn.org/stable/modules/cross_validation.html\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "\n",
    "cv=10\n",
    "n_neighbors = 15\n",
    "\n",
    "#10-fold cross validation on k-nearest neighbors\n",
    "#TOFILL: \n",
    "\n",
    "knn_clf = \n",
    "y_cv_pred ="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use these predictions to compute the accuracy of your 10-fold cross validation on your 15-nearest neighbors model :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "#TOFILL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$\\approx$ 82.5%, pretty good for someone who doesn't know about medicine ! However accuracy is a quite limited view of the prediction abilities of your classifier. Indeed it doesn't make the difference between patients that have been incorrectly classified with a heart disease (because they were healthy) with patients that have been incorrectly classified as healthy (because they had a disease).\n",
    "\n",
    "Let's look at the *confusion matrix* of the prediction, it gives use the number of patients that have been correctly classified as having a disease : the *true positives* (TP) ; the number of patients that have been correctly classified as not having a disease : the *true negatives* (TN); the number of patients that have been incorrectly classified as having a disease : the *false positives* (FP) ; and the number of patients that have been incorrectly classified as nothaving a disease : the *false negatives* (FN). To remember more easily, remark that the true/false refers to the *true* class of the test samples, whereas the positive/negative refers to the *predicted* class by the classifier.\n",
    "\n",
    "The confusion matrix gives these four numbers in the following format:\n",
    "\n",
    "|  |  |\n",
    "|--|--|\n",
    "|TN|FP|\n",
    "|FN|TP|\n",
    "\n",
    "The accuracy is computed by :\n",
    "\n",
    "<div style=\"font-size: 150%;\" align= \"center\"> \n",
    "    accuracy $= \\frac{TP + TN}{TP + TN + FP + FN}$\n",
    "</div>\n",
    "\n",
    "One can also look at the accuracy of positive predictions, called the *precision*:\n",
    "\n",
    "<div style=\"font-size: 150%;\" align= \"center\"> \n",
    "    precision $= \\frac{TP }{TP + FP}$\n",
    "</div>\n",
    "\n",
    "Or at the ratio of positive samples correctly detected by the classifier, called the *recall*:\n",
    "\n",
    "<div style=\"font-size: 150%;\" align= \"center\"> \n",
    "    recall $= \\frac{TP }{TP + FN}$\n",
    "</div>\n",
    "\n",
    "These two metrics are often grouped together as a single one called the *f1-measure*:\n",
    "\n",
    "<div style=\"font-size: 150%;\" align= \"center\"> \n",
    "    F1 $=  2 \\times \\frac{precision \\times recall}{precision + recall}$\n",
    "</div>\n",
    "\n",
    "You can read more about it in chapter 3 of the book *Hands on Machine Learning with scikit-learn and tensorflow*.\n",
    "\n",
    "So these metrics gives use different informations about our classifier predictive performances, we are gonna see in more details how below, first compute them on the results of your cross-validation :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Showing the confusion matrix of the cross-validation\n",
    "from sklearn.metrics import confusion_matrix,  precision_score, recall_score, accuracy_score, f1_score\n",
    "#TOFILL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the memo.txt file to answer these questions:\n",
    "\n",
    "2)a) How many patient were incorrectly diagnosed with a Heart disease ?\n",
    "\n",
    "2)b) How many patient were incorrectly diagnosed as being Healthy ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general, remember that accuracy is more sensitive to class imbalance (which is not the case here as there is roughly as many positive as negative samples in the data), and that the f1 better summarizes compromize between precision and recall, and is generally preferred to accuracy when reporting a single metric on predicted classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The precision/recall trade-off"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have assessed some measures that are applicables to the predicted classes, i.e. 0 or 1. Remember that KNN assigns a probability to each test sample, and simply assigns a class if this probability is higher than > 0.5:\n",
    "\n",
    "<div style=\"font-size: 150%;\" align= \"center\"> \n",
    "$y'= 1$ if and only if $P(y' = 1 ) = \\frac{1}{k} \\sum_{ i \\in 1,\\dots,k} \\mathbb{1}(y_i = 1)  > 0.5$\n",
    "</div>\n",
    "\n",
    "Let us say we are here performing a first diagnosis test with our classifier, for further medical investigation if the prediction is positive. In this context, it is much more important to not say someone is healthy if he is not, rather than saying someone is sick if he is not (which can be discovered with later medical tests). In other words, we want to have a few false negatives, even if that implies having more false positives. This means we'd prefer to have a higher Recall, to the cost of having a lower Precision. And that implies choosing a threshold that is lower than 0.5 for assigning the classes.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the *cross_val_predict* to get the probability of each sample through a 10-fold cross-validation, check the *method* argument to do so, and print these probabilities:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOFILL\n",
    "y_cv_proba = \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's check that if we apply a 0.5 threshold, we obtain the same predictions as before:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#By default KNN applies a 0.5 threshold to do its predictions:\n",
    "((y_cv_proba > 0.5).astype(int) == y_cv_pred).all()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's plot the different values of Precision and Recall for different thresholds:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Precision recall tradeoff\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "def plot_precision_recall_vs_threshold(precisions, recalls, thresholds):\n",
    "    plt.plot(thresholds, precisions[:-1], \"b--\", label=\"Precision\")\n",
    "    plt.plot(thresholds, recalls[:-1], \"g-\", label=\"Recall\")\n",
    "    plt.xlabel(\"Threshold\")\n",
    "    plt.legend(loc=\"upper left\")\n",
    "    plt.ylim([0, 1])\n",
    "\n",
    "#TOFILL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the lower the probability threshold, the higher the recall and the lower the precision. There is always a cost: this is what is called the precision/recall tradeoff. A classic way of seeing that is to plot the so-called precision-recall curve, with recall as abcissa and precision in ordinate. Plot it :"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plot_precision_recall_curve(precisions, recalls):\n",
    "    plt.step(recalls, precisions, linewidth=2)\n",
    "    plt.xlabel('Recall')\n",
    "    plt.ylabel('Precision')\n",
    "    \n",
    "#TOFILL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here again, we can see the precision/recall trade-off. The better the classifier is, the closer the curve will be to the top-right corner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, let's not forget we are trying to diagnose people's heart disease. Let's say we are ready to accept to incorrectly label as healthy at most 1% of the patients that have a heart disease (false negatives), i.e. we want a recall of 0.99.\n",
    "Given the threshold curve above, choose a threshold that approximately yields a recall of 0.99, and recompute the confusion matrix as well as precision:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOFILL\n",
    "threshold =\n",
    "y_cv_pred_99recall = y_cv_proba > threshold\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the memo.txt file to answer these two questions:\n",
    "\n",
    "3)a)What is the precision if we change the threshold to have a 0.99 recall ? \n",
    "\n",
    "b) How many patient were incorrectly diagnosed as being Healthy (false negatives)?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Receiver Operating Characteristic (ROC) curve"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(Yet) another way to look at your classfier performance, is to consider the false positive rate instead of precision:\n",
    "\n",
    "<div style=\"font-size: 120%;\" align= \"center\"> \n",
    "False Positive Rate (FPR) $= \\frac{FP}{FP + TN}$\n",
    "</div>\n",
    "\n",
    "This is the proportion of negative test samples that are incorrectly classified as positive. Plotting the FPR against the recall gives another classic evaluation curve, the ROC curve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#The ROC curve\n",
    "from sklearn.metrics import roc_curve\n",
    "\n",
    "def plot_roc_curve(fpr, tpr, label=None):\n",
    "    plt.plot(fpr, tpr, linewidth=2, label=label)\n",
    "    plt.plot([0, 1], [0, 1], 'k--')\n",
    "    plt.axis([0, 1, 0, 1])\n",
    "    plt.xlabel('False Positive Rate')\n",
    "    plt.ylabel('True Positive Rate (=Recall)')\n",
    "\n",
    "#TOFILL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here again there is a trade-off between TPR and FPR. However instead of choosing a threshold that corresponds to a single point on this curve as we did before to compute our metrics, we can compute the area under this curve (AUC). The ROC curve is always monotonically increasing and thus is a good indication that if you have a higher area under curve, you can probably find a better TPR/FPR trade-off. Computing the area under the curve is thus a metric that can compare classifiers for all possible thresholds at once. Compute it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "#TOFILL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that this is the preferred metric overall the other metrics to evaluate your classifiers, however it requires to have access to the probability given to each test sample. Also note that when there is really few positives, we prefer to calculate the area under the precision/recall curve that we have seen before, which is also called*\"average precision* (see sklearn.metrics.average_precision_score). Indeed the ROC-AUC can be biased if the dataset is too imbalanced."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you Want to know more about all classification metrics, you can read this: https://neptune.ai/blog/f1-score-accuracy-roc-auc-pr-auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill the memo.txt file to answer the following questions:\n",
    "\n",
    "4) Choosing a metric:\n",
    "\n",
    "a) If I can compute my test sample probabilities, which overall metric should I use ?\n",
    "\n",
    "b) And if I have a lot more negatives than positives in the dataset ?\n",
    "\n",
    "c) And if I only have the class predictions and no probabilities ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hyper-parameters search : the best number of neighbors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we know what's the best way of comparing classifiers, we can start choosing the best hyper-parameter k. You have already studied the bias/variance tradeoff with linear regression. Here this is the same: with k=1 the bias will be 0, but the variance very high, and conversely with too high a k. In both cases, we will have a sub-optimal ROC-AUC.\n",
    "\n",
    "You can read more about the bias/variance trade-off with KNNs here: https://medium.com/30-days-of-machine-learning/day-3-k-nearest-neighbors-and-bias-variance-tradeoff-75f84d515bdb\n",
    "\n",
    "\n",
    "Let's search for the value of k that gives the best ROC AUC score, by doing a grid-search as you already did with linear regression, but first lets play a bit with the value of k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot the ROC curve and compute the ROC AUC with k=3, 15 and 50:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOFILL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now plot the ROC AUC for each k between 1 and 40:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Plot ROC AUC score against the number of neighbors from 1 to 40\n",
    "def plot_roc_aucs(knn_roc_aucs, k_range):\n",
    "    plt.plot([k for k in k_range], knn_roc_aucs, color = 'red')\n",
    "    plt.xticks([i for i in k_range])\n",
    "    plt.xlabel('Number of Neighbors (K)')\n",
    "    plt.ylabel('Scores')\n",
    "    plt.title('K Neighbors Classifier ROC-AUC for different K values')\n",
    "\n",
    "\n",
    "knn_roc_aucs = []\n",
    "k_range =  range(1,41)\n",
    "\n",
    "#TOFILL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can see, the optimal value of k is somewhere in the middle. To automatically select the k value that yields the best ROC AUC, use the *GridSearchCV* class to perform a grid search, and print the best ROC AUC and the best k value:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Grid search k and give its roc auc score:\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "param_grid =  {'n_neighbors': k_range}\n",
    "knn_clf = KNeighborsClassifier()\n",
    "\n",
    "#TOFILL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's retrain a model with the optimal value of k obtained and find a theshold that gives a recall of approximately 0.99. Recompute the precision now."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOFILL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Optional : Try out other classification models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Trees "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Decision trees can be very useful when you need to understand how the classifier chooses the classes. You can read more about it there:\n",
    "\n",
    "https://en.wikipedia.org/wiki/Decision_tree_learning\n",
    "\n",
    "https://scikit-learn.org/stable/modules/tree.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a decision tree with *max_depth=3*, and use the  sklearn.tree.plot_tree function to visualize it (use the *feature_names* and *class_names* parameters to have useful infos in the tree visualization)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import tree\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "max_depth = 3\n",
    "\n",
    "#TOFILL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Grid search over the 'max_depth' parameter and compute the the best ROC-AUC for the decision tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOFILL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Support Vector Machines"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In general Support Vector Machines (SVM) is the classification method that often gives the best predictive performances. You can read about it there:\n",
    "\n",
    "https://scikit-learn.org/stable/modules/svm.html (beginning of section 1.4.7 gives an intuitive view of the principles of SVMs)\n",
    "\n",
    "Similarly, grid search over the 'C' parameter and compare ROC AUC scores:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import SVC\n",
    "#TOFILL\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "SVMs offer the possibility to express complex combinations of the features, through different *kernels*. See:\n",
    "\n",
    "https://scikit-learn.org/stable/auto_examples/svm/plot_svm_kernels.html\n",
    "\n",
    "https://towardsdatascience.com/understanding-support-vector-machine-part-2-kernel-trick-mercers-theorem-e1e6848c6c4d\n",
    "\n",
    "Check the available kernels in scikit-learn, and do a grid-search on kernel types and other hyper-parameters of your choice."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOFILL"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Select the best model among all class of models and their hyper-parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Write a function that takes a list of different scikit-learn classifiers, as well as a list of each parameter grid to be searched over each classifier, and a scoring function ; and returns the best classifier along with its best parameters and its score. Finally, test it with KNN, SVM and DecisionTrees, and ensure you get a result that is consistent with the results previously obtained. You can also test with LogisticRegression, another powerful classification model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def select_best_model_overall(classifiers_list, param_grids_list, X, y, cv=10, scoring='roc_auc'):\n",
    "    #TOFILL\n",
    "    \n",
    "    return best_classifier, best_parameters, best_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#TOFILL"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
